# P8.1: Performance Analysis and Optimization Report

## 1. Introduction and Goals

Milestone P8.1, as defined in `docs/README.md`, focuses on "性能分析与优化" (Performance Analysis and Optimization) for the PSOC application. The primary goals are:

*   Identify performance bottlenecks using analysis tools (conceptually, `perf`, `flamegraph`).
*   Optimize rendering and image processing algorithms, leveraging parallelism (e.g., `rayon`) and potentially SIMD.
*   Optimize memory usage.

This report details the analysis performed, one sample optimization implemented, and recommendations for further performance enhancements.

## 2. Areas Analyzed for Performance

Based on code review and common performance hotspots in image editing applications, the following areas were prioritized for analysis:

*   **Core Rendering Pipeline (`crates/psoc-core/src/rendering.rs`):**
    *   Layer compositing logic (`RenderEngine::composite_layer_sequential` and `composite_layer_parallel`).
    *   Application of adjustment layers, especially concerning opacity handling (`RenderEngine::apply_adjustment_layer`).
*   **Pixel Data Operations (`crates/psoc-core/src/pixel.rs`):**
    *   Efficiency of `PixelData` creation, access (`get_pixel`, `set_pixel`), and cloning.
*   **Image Adjustments and Filters (`crates/psoc-core/src/adjustments/`):**
    *   Specific filter implementations like `GaussianBlurFilter` (`filters/blur.rs`).
    *   Tone adjustments like `CurvesAdjustment` (`curves.rs`).
    *   Loops performing per-pixel operations.
*   **Smart Object Handling (`crates/psoc-core/src/smart_object.rs` and its usage in `RenderEngine`):**
    *   Rendering of embedded or linked content.
    *   Application of non-destructive transforms.
*   **Memory Usage:**
    *   Large data structures (`PixelData`, `Document`, `Layer`).
    *   Frequent allocations or cloning of these structures.

## 3. Profiling Methodology (Simulated)

Direct execution of profiling tools like `perf` or `flamegraph` was not performed in this environment. Instead, a simulated profiling approach was adopted:

1.  **Code Review:** Critical code paths were manually inspected, looking for computationally intensive loops, frequent allocations, or potentially inefficient algorithmic patterns.
2.  **Hypothesis Generation:** Based on common Rust performance characteristics and typical image processing bottlenecks, hypotheses were formed about where CPU time and memory would be most consumed.
    *   **CPU Hotspots:** Expected in deeply nested loops (e.g., per-pixel processing within image filters), complex mathematical calculations (e.g., some blend modes), and frequent function calls in tight loops.
    *   **Memory Hotspots:** Expected from creation and cloning of large `PixelData` buffers, and potentially large `CommandHistory` if commands store substantial data.
3.  **Focus on Parallelism & Data Structures:** Particular attention was paid to identifying sections of code that were "embarrassingly parallel" (where operations on different data chunks are independent) and thus good candidates for `rayon`. The use and cloning of large data structures like `PixelData` were also scrutinized.

## 4. Identified Bottlenecks (Hypothesized)

The simulated profiling and code review suggested the following as likely performance bottlenecks:

*   **Sequential Per-Pixel Loops:** Many image adjustment and filter algorithms (`CurvesAdjustment::apply`, `GaussianBlurFilter::blur_horizontal/blur_vertical`) iterate over all pixels sequentially. These are CPU-bound and do not leverage multi-core processors effectively.
*   **`PixelData` Cloning for Opacity Adjustments:** `RenderEngine::apply_adjustment_layer` clones the entire intermediate `PixelData` result when an adjustment layer has an opacity less than 1.0. For large images, this is a significant memory allocation and copy overhead.
*   **Layer Compositing Details:** While `RenderEngine::composite_layer_parallel` uses `rayon` for tile-based processing, the collection of `tile_updates` and their sequential application could still be a point of contention or high memory churn for very numerous/small updates.
*   **Smart Object Rendering:** Depending on the complexity of `SmartObjectManager` and its caching, rendering smart objects (especially those requiring re-rasterization or external file access) can be demanding.

## 5. Implemented Optimization: Parallelizing `CurvesAdjustment::apply()`

As a sample optimization, the `apply` method of `CurvesAdjustment` was parallelized using `rayon`.

*   **File Modified:** `crates/psoc-core/src/adjustments/curves.rs`
*   **Change Description:**
    *   The primary change was to the `apply` method when handling the `PixelData::Rgba(ref mut array)` variant.
    *   The sequential nested loops (`for y in 0..height { for x in 0..width { ... } }`) were replaced with a parallel iteration over the rows of the `ndarray::Array3`.
    *   `rayon::prelude::*` and `ndarray::Axis` were imported.
    *   The implementation now uses `array.axis_iter_mut(Axis(0)).into_par_iter().for_each(|mut row| { ... });` to process image rows in parallel.
    *   Inside the closure for each row, it iterates sequentially over the pixels in that row (which are contiguous in memory for a row), applying the `self.apply_to_pixel_internal()` logic. Each channel (`r`, `g`, `b`) is updated directly in the mutable row view.
*   **Code Snippet (Conceptual - illustrating the change):**

    **Before (Sequential):**
    ```rust
    // fn apply(&self, pixel_data: &mut PixelData) -> Result<()> {
    //     if self.is_identity() { return Ok(()); }
    //     let (width, height) = pixel_data.dimensions();
    //     if let PixelData::Rgba(ref mut array) = pixel_data {
    //         for y in 0..height {
    //             for x in 0..width {
    //                 let current_pixel = RgbaPixel::new(
    //                     array[[y as usize, x as usize, 0]],
    //                     // ... extract g,b,a
    //                 );
    //                 let adjusted_pixel = self.apply_to_pixel_internal(current_pixel);
    //                 array[[y as usize, x as usize, 0]] = adjusted_pixel.r;
    //                 // ... set g,b,a
    //             }
    //         }
    //     } // ... else Raw data
    //     Ok(())
    // }
    ```

    **After (Parallelized for `PixelData::Rgba`):**
    ```rust
    use rayon::prelude::*;
    use ndarray::Axis;
    // ...
    // fn apply(&self, pixel_data: &mut PixelData) -> Result<()> {
    //     if self.is_identity() { return Ok(()); }
    //     match pixel_data {
    //         PixelData::Rgba(ref mut array) => {
    //             array.axis_iter_mut(Axis(0)) // Iterate over rows (Axis(0))
    //                 .into_par_iter()        // Convert to parallel iterator
    //                 .for_each(|mut row| {   // Process each row in parallel
    //                     for x in 0..row.shape()[0] { // Iterate pixels in this row (row is ArrayViewMut2 - (width, channels))
    //                         let current_pixel = RgbaPixel::new(
    //                             row[[x, 0]], // row is now 2D (width, channels)
    //                             row[[x, 1]],
    //                             row[[x, 2]],
    //                             row[[x, 3]],
    //                         );
    //                         let adjusted_pixel = self.apply_to_pixel_internal(current_pixel);
    //                         row[[x, 0]] = adjusted_pixel.r;
    //                         row[[x, 1]] = adjusted_pixel.g;
    //                         row[[x, 2]] = adjusted_pixel.b;
    //                         // Alpha is preserved by curves
    //                     }
    //                 });
    //         }
    //         PixelData::Raw { .. } => { // Sequential fallback for Raw
    //             // ... original sequential loop ...
    //             // TODO: Parallelize PixelData::Raw if it becomes a performance concern
    //         }
    //     }
    //     Ok(())
    // }
    ```
*   **Benchmark Added:** A new benchmark function `benchmark_curves_adjustment` was added to `benches/image_processing.rs`. This benchmark creates a 1024x1024 `PixelData::Rgba` object, applies a non-identity `CurvesAdjustment` to a clone of it in each iteration, and measures the execution time using `criterion`.
*   **Expected Outcome:** This change is expected to provide a significant speedup for the Curves adjustment on multi-core systems, as the per-pixel calculations are now distributed across available CPU cores. The actual speedup factor would depend on the number of cores and overhead of parallelization.

## 6. Proposed Further Optimization Strategies

1.  **Parallelize `GaussianBlurFilter`:**
    *   **Target:** `blur_horizontal()` and `blur_vertical()` methods in `crates/psoc-core/src/adjustments/filters/blur.rs`.
    *   **Strategy:** Use `rayon` to parallelize the outer loops (iteration over rows for horizontal pass, columns for vertical pass). Each thread would compute a block of rows/columns independently. The current two-pass approach with an intermediate buffer (`temp_data`) is conducive to this.
    *   **Challenge:** Ensuring mutable access to the output buffer (`output` or `pixel_data` in the second pass) is handled correctly by `rayon` (e.g., by giving each thread mutable slices to distinct parts of the output).

2.  **Optimize `PixelData` Cloning in `RenderEngine::apply_adjustment_layer`:**
    *   **Target:** The `result.clone()` call when an adjustment layer has opacity < 1.0.
    *   **Strategy 1 (Preferred if feasible): Per-Pixel Blended Application.**
        *   Modify the `Adjustment` trait or add a new one. Instead of `apply(&mut PixelData)`, have a method like `preview_pixel(&self, original: RgbaPixel) -> RgbaPixel` that returns what the fully adjusted pixel would be.
        *   The `RenderEngine` would then iterate over pixels, get the original pixel, call `preview_pixel`, and then use `RgbaPixel::lerp(original, previewed_adjusted, opacity)` to compute the final pixel, writing it directly. This avoids any full buffer clone.
    *   **Strategy 2 (If full buffer ops are needed by some adjustments): Tile-Based Cloning.**
        *   If an adjustment *must* operate on a temporary buffer (e.g., complex spatial filters not easily done per-pixel), clone only the *current tile* being processed by `RenderEngine`'s parallel pipeline, apply the adjustment to this small tile-clone, blend, and then write back. This drastically reduces the size of cloned data.

3.  **Generalize Parallelism for Adjustments:**
    *   Many other adjustments in `crates/psoc-core/src/adjustments/` likely involve per-pixel loops (e.g., HSL, Brightness, Contrast). A common pattern or helper function could be developed to easily parallelize these using `rayon`, similar to the `CurvesAdjustment` optimization.

4.  **SIMD Exploration:**
    *   For operations within tight loops that process pixel channels (e.g., blending functions, per-channel adjustments), investigate the use of explicit SIMD instructions (e.g., via `std::simd` or Rust SIMD wrapper crates like `packed_simd_2` or by ensuring `ndarray` uses them). This can provide further speedups beyond `rayon`'s task-based parallelism by exploiting data-level parallelism within each core. This is a more advanced optimization.

5.  **Memory Profiling and Reduction:**
    *   Use tools like `heaptrack` or Valgrind's `massif` (conceptually) to identify sources of large allocations or memory churn.
    *   Review data structures for opportunities to reduce size or use more memory-efficient representations where possible without sacrificing too much performance (e.g., for `CommandHistory` entries).

## 7. Conclusion

The PSOC codebase has several clear opportunities for performance optimization, primarily through targeted parallelization of existing sequential algorithms using `rayon` and by addressing memory allocation hotspots like large buffer cloning. The successful parallelization of `CurvesAdjustment::apply()` serves as a proof-of-concept for this approach. Continuous benchmarking and profiling will be essential to guide further optimization efforts as the project evolves.
